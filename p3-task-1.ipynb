{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5093016,"sourceType":"datasetVersion","datasetId":2957522}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Libraries","metadata":{}},{"cell_type":"code","source":"# %%capture\n# 1. INSTALL NECESSARY LIBRARIES\n!pip install transformers[torch] datasets accelerate evaluate rouge_score nltk scikit-learn\n\n# 2. IMPORTS\nimport torch\nimport pandas as pd\nimport numpy as np\nimport ast  # For safely evaluating string representations of lists\nimport evaluate  # For ROUGE and BLEU\nimport math # For calculating steps per epoch\nfrom datasets import load_dataset, Dataset, DatasetDict, ClassLabel, Features\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 3. GLOBAL CONFIGURATION AND CONSTANTS\n# --- Parameters You Can Tune ---\nMODEL_NAME = 'gpt2'\nFILE_PATH = '/kaggle/input/3a2mext/3A2M_EXTENDED.csv'\n\n# Training parameters\nTRAIN_EPOCHS = 3\nFP16_TRAINING = True\nBATCH_SIZE = 4\nGRAD_ACC_STEPS = 8\nMAX_LENGTH = 512\n\n# Data parameters\nDATA_SUBSAMPLE = 50000\nVAL_SIZE = 0.1  # 10% for validation\nTEST_SIZE = 0.1  # 10% for test (80% for train)\n# ----------------------------------\n\nprint(\"--- Configuration Loaded ---\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Data Subsample: {DATA_SUBSAMPLE} rows\")\nprint(f\"Split: 80% Train, 10% Validation, 10% Test\")\nprint(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACC_STEPS}\")\nprint(\"----------------------------\")\n\n# 4. LOAD MODEL AND TOKENIZER\nprint(\"\\n--- Loading Model and Tokenizer ---\")\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))\nprint(\"Model and Tokenizer loaded successfully.\")\n\n# 5. LOAD, CLEAN, AND DEDUPLICATE DATASET\nprint(\"\\n--- Loading, Cleaning, and Deduplicating Dataset ---\")\ntry:\n    raw_dataset = load_dataset('csv', data_files=FILE_PATH, split='train')\n    print(f\"Original dataset size: {len(raw_dataset)}\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}. Please check the file path.\")\n    data = {'title': ['Test 1', 'Test 2', 'Test 3', 'Test 4'],\n            'Extended_NER': [\"['a']\", \"['b']\", \"['c']\", \"['d']\"],\n            'directions': [\"['step 1']\", \"['step 2']\", \"['step 3']\", \"['step 4']\"],\n            'genre': ['vegetables', 'sides', 'nonveg', 'vegetables']}\n    raw_dataset = Dataset.from_dict(data)\n\nif DATA_SUBSAMPLE > 0 and DATA_SUBSAMPLE < len(raw_dataset):\n    raw_dataset = raw_dataset.shuffle(seed=42).select(range(DATA_SUBSAMPLE))\n    print(f\"Subsampled dataset to {len(raw_dataset)} rows.\")\n\noriginal_count = len(raw_dataset)\nraw_dataset = raw_dataset.filter(\n    lambda x: x['title'] is not None and \\\n              x['Extended_NER'] is not None and \\\n              x['directions'] is not None and \\\n              x['genre'] is not None\n)\nprint(f\"Filtered {original_count - len(raw_dataset)} rows with null values.\")\n\ndef clean_genre(example):\n    example['genre'] = str(example['genre']).lower().strip()\n    return example\nraw_dataset = raw_dataset.map(clean_genre)\nprint(\"Normalized 'genre' column.\")\n\ndf = raw_dataset.to_pandas()\noriginal_count = len(df)\ndf.drop_duplicates(subset=['title', 'Extended_NER', 'directions'], inplace=True, keep='first')\nprint(f\"Removed {original_count - len(df)} duplicate recipes.\")\nraw_dataset = Dataset.from_pandas(df)\nprint(f\"Final cleaned dataset size: {len(raw_dataset)}\")\n\n# 6. STRATIFIED 80/10/10 SPLIT\nprint(\"\\n--- Performing Stratified 80/10/10 Split ---\")\n\n# --- FIX: Cast 'genre' to ClassLabel for stratification ---\nunique_genres = raw_dataset.unique(\"genre\")\nprint(f\"Found {len(unique_genres)} unique genres for stratification.\")\ncurrent_features = raw_dataset.features\nnew_features = Features({\n    **current_features,\n    \"genre\": ClassLabel(names=unique_genres)\n})\nprint(\"Casting 'genre' column to ClassLabel type...\")\nstratify_ready_dataset = raw_dataset.cast(new_features)\nprint(\"Casting complete.\")\n# --- End of Fix ---\n\ntrain_test_split = stratify_ready_dataset.train_test_split(\n    test_size=(VAL_SIZE + TEST_SIZE),\n    seed=42,\n    stratify_by_column=\"genre\"\n)\nval_test_split = train_test_split['test'].train_test_split(\n    test_size=0.5,\n    seed=42,\n    stratify_by_column=\"genre\"\n)\nsplit_dataset = DatasetDict({\n    'train': train_test_split['train'],\n    'validation': val_test_split['train'],\n    'test': val_test_split['test']\n})\nprint(f\"Train split size: {len(split_dataset['train'])}\")\nprint(f\"Validation split size: {len(split_dataset['validation'])}\")\nprint(f\"Test split size: {len(split_dataset['test'])}\")\n\n# 7. DEFINE FORMATTING FUNCTION\ndef format_and_normalize_record(example):\n    try:\n        ingredients_list = ast.literal_eval(example['Extended_NER'])\n        if not isinstance(ingredients_list, list): ingredients_list = []\n    except:\n        ingredients_list = []\n    ingredients_str = \", \".join(filter(None, [ing.lower().strip() for ing in ingredients_list]))\n\n    try:\n        directions_list = ast.literal_eval(example['directions'])\n        if not isinstance(directions_list, list): directions_list = []\n    except:\n        directions_list = []\n    directions_str = \" \".join(f\"{i+1}. {step.lower().strip()}\" for i, step in enumerate(directions_list))\n\n    title_str = str(example['title']).strip().lower()\n\n    if not title_str or not ingredients_str or not directions_str:\n        return {\"text\": None}\n\n    text = (\n        f\"TITLE: {title_str}\\n\"\n        f\"INGREDIENTS: {ingredients_str}\\n\"\n        f\"RECIPE: {directions_str}\"\n        f\"{tokenizer.eos_token}\"\n    )\n    return {\"text\": text}\n\n# 8. APPLY FORMATTING\nprint(\"\\nApplying formatting and normalization to all splits...\")\nformatted_dataset = split_dataset.map(\n    format_and_normalize_record,\n    num_proc=4,\n    remove_columns=split_dataset['train'].column_names\n)\nformatted_dataset = formatted_dataset.filter(\n    lambda example: example['text'] is not None\n)\n\n# 9. DEFINE TOKENIZATION FUNCTION (WITH PADDING FIX)\nprint(\"\\n--- Defining Tokenization ---\")\n\ndef tokenize_function(examples):\n    # --- FIX: Padding is applied here to avoid collator errors ---\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",  # Pad all to max_length\n        max_length=MAX_LENGTH,\n    )\n    \n# 10. APPLY TOKENIZATION AND SET LABELS\nprint(\"\\n--- Tokenizing Dataset (all splits) ---\")\ntokenized_dataset = formatted_dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=[\"text\"]\n)\n\ndef set_labels(examples):\n    examples[\"labels\"] = examples[\"input_ids\"].copy()\n    return examples\ntokenized_dataset = tokenized_dataset.map(set_labels, batched=True, num_proc=4)\nprint(\"Tokenization and label setting complete.\")\n\n# 11. CONFIGURE TRAINER (WITH ALL VERSIONING FIXES)\nprint(\"\\n--- Configuring Trainer ---\")\n\n# --- FIX: DataCollator MUST NOT have padding arguments ---\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n    # Padding is now handled in the tokenizer (Step 9)\n)\n\n# --- Calculate steps per epoch for old args ---\ntry:\n    total_train_samples = len(tokenized_dataset[\"train\"])\n    effective_batch_size = BATCH_SIZE * GRAD_ACC_STEPS\n    STEPS_PER_EPOCH = math.ceil(total_train_samples / effective_batch_size)\n    print(f\"Calculated steps per epoch: {STEPS_PER_EPOCH}\")\nexcept:\n    print(\"Warning: Could not calculate steps_per_epoch. Defaulting to 500.\")\n    STEPS_PER_EPOCH = 500\n\n# --- FIX: TrainingArguments uses old names and workarounds ---\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-recipe-generator\",\n    overwrite_output_dir=True,\n    num_train_epochs=TRAIN_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACC_STEPS,\n    fp16=FP16_TRAINING,\n    \n    # --- Arguments for old library versions ---\n    do_eval=True,\n    eval_steps=STEPS_PER_EPOCH,\n    save_steps=STEPS_PER_EPOCH,\n    logging_steps=STEPS_PER_EPOCH,\n    \n    # --- Fix for Kaggle hanging ---\n    dataloader_num_workers=0,\n    \n    # --- Removed load_best_model_at_end to avoid final error ---\n    \n    learning_rate=5e-5,\n    warmup_steps=200,\n    weight_decay=0.01,\n    report_to=\"none\",\n)\n\n# 12. INITIALIZE TRAINER\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n)\nprint(\"Trainer initialized.\")\n\n# 13. TRAIN THE MODEL (WITH BETTER ERROR REPORTING)\nprint(\"\\n--- Starting Model Training ---\")\ntry:\n    trainer.train()\n    print(\"--- Training Finished ---\")\n\n    print(\"Saving final model...\")\n    trainer.save_model(\"./final_model\")\n    tokenizer.save_pretrained(\"./final_model\")\n    print(\"Model saved to ./final_model\")\n\nexcept Exception as e:\n    # --- FIX: More accurate error reporting ---\n    print(f\"\\n--- AN ERROR OCCURRED DURING TRAINING ---\")\n    print(f\"ERROR: {e}\")\n    print(\"\\nThis error is often due to:\")\n    print(\"1. An Out-of-Memory (OOM) error. Try reducing BATCH_SIZE or MAX_LENGTH.\")\n    print(\"2. A data collation error (check tokenizer padding).\")\n    print(\"3. A mismatch in CUDA/PyTorch versions.\")\n\n# 14. LOAD FINE-TUNED MODEL FOR INFERENCE\nprint(\"\\n--- DELIVERABLE: Sample Generations (Qualitative) ---\")\ntry:\n    final_model = GPT2LMHeadModel.from_pretrained(\"./final_model\")\n    final_tokenizer = GPT2Tokenizer.from_pretrained(\"./final_model\")\n    print(\"Loaded final-trained model from ./final_model\")\nexcept:\n    print(\"Could not load fine-tuned model. Using base model for generation.\")\n    final_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n    final_tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n    final_tokenizer.pad_token = final_tokenizer.eos_token\n\nrecipe_generator = pipeline(\n    \"text-generation\",\n    model=final_model,\n    tokenizer=final_tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# 15. DEFINE GENERATION PROMPTS\nprompt1 = (\n    \"title: spicy chicken pasta\\n\"\n    \"ingredients: chicken breast, pasta, cayenne pepper, olive oil, garlic, tomatoes\\n\"\n    \"recipe:\"\n)\nprompt2 = (\n    \"title: black bean and turkey chili\\n\"\n    \"ingredients:\\n\"\n    \"recipe:\"\n)\nprompt3 = (\n    \"title:\\n\"\n    \"ingredients: arugula, pomegranate arils, persimmon, feta cheese, walnuts, vinaigrette\\n\"\n    \"recipe:\"\n)\nprompts = [prompt1, prompt2, prompt3]\n\n# 16. RUN QUALITATIVE GENERATION EXAMPLES\nfor i, prompt in enumerate(prompts):\n    print(f\"\\n--- Generation Example {i+1} ---\")\n    print(f\"PROMPT:\\n{prompt}\\n\")\n    \n    generated_text = recipe_generator(\n        prompt,\n        max_length=300,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95,\n        pad_token_id=final_tokenizer.eos_token_id\n    )\n    \n    print(\"GENERATED RECIPE:\")\n    full_text = generated_text[0]['generated_text']\n    generated_part = full_text[len(prompt):]\n    print(generated_part)\n\n# 17. SETUP QUANTITATIVE EVALUATION (ON TEST SET)\nprint(\"\\n--- DELIVERABLE: Metric Evaluation (ROUGE, BLEU) ---\")\nprint(\"*** Running evaluation on the 10% held-out TEST set ***\")\n\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\n\neval_sample_size = 100\nif len(tokenized_dataset[\"test\"]) > eval_sample_size:\n    eval_sample = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(eval_sample_size))\nelse:\n    eval_sample = tokenized_dataset[\"test\"]\n    \nprint(f\"Running quantitative evaluation on {len(eval_sample)} test samples...\")\npredictions = []\nreferences = []\n\n# 18. RUN QUANTITATIVE EVALUATION\nfor example in eval_sample:\n    token_ids = example['input_ids']\n    full_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n    \n    try:\n        parts = full_text.split(\"recipe:\")\n        prompt_text = parts[0] + \"recipe:\"\n        reference_text = parts[1].strip()\n    except:\n        continue\n\n    generated_output = recipe_generator(\n        prompt_text,\n        max_length=len(token_ids) + 50,\n        pad_token_id=final_tokenizer.eos_token_id\n    )\n    \n    full_gen_text = generated_output[0]['generated_text']\n    predicted_text = full_gen_text[len(prompt_text):].strip()\n    \n    predictions.append(predicted_text)\n    references.append(reference_text)\n\nprint(\"Evaluation generation complete. Calculating scores...\")\n\n# 19. DISPLAY EVALUATION RESULTS AND FINAL NOTES\nif predictions:\n    rouge_scores = rouge.compute(predictions=predictions, references=references)\n    bleu_scores = bleu.compute(predictions=predictions, references=references)\n\n    print(\"\\n--- Final Evaluation Results (on TEST set) ---\")\n    print(\"\\nROUGE Scores:\")\n    print(f\"  ROUGE-1: {rouge_scores['rouge1'] * 100:.2f}\")\n    print(f\"  ROUGE-2: {rouge_scores['rouge2'] * 100:.2f}\")\n    print(f\"  ROUGE-L: {rouge_scores['rougeL'] * 100:.2f}\")\n\n    print(\"\\nBLEU Score:\")\n    print(f\"  BLEU: {bleu_scores['bleu'] * 100:.2f}\")\n\n    print(\"\\n--- Human Evaluation Notes (for your report) ---\")\n    print(\"1. **Coherence:** Does the recipe make logical sense?\")\n    print(\"2. **Relevance:** Does it use the ingredients from the prompt?\")\n    print(\"3. **Fluency:** Is the text grammatically correct and readable?\")\nelse:\n    print(\"No predictions were generated. Skipping metric evaluation.\")\n\nprint(\"\\n--- SCRIPT COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:32:26.720854Z","iopub.execute_input":"2025-11-01T13:32:26.721172Z","iopub.status.idle":"2025-11-01T15:57:54.182085Z","shell.execute_reply.started":"2025-11-01T13:32:26.721152Z","shell.execute_reply":"2025-11-01T15:57:54.181422Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\nRequirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.8.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers[torch]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\n--- Configuration Loaded ---\nModel: gpt2\nData Subsample: 50000 rows\nSplit: 80% Train, 10% Validation, 10% Test\nEffective Batch Size: 32\n----------------------------\n\n--- Loading Model and Tokenizer ---\nModel and Tokenizer loaded successfully.\n\n--- Loading, Cleaning, and Deduplicating Dataset ---\nOriginal dataset size: 2231143\nSubsampled dataset to 50000 rows.\nFiltered 0 rows with null values.\nNormalized 'genre' column.\nRemoved 0 duplicate recipes.\nFinal cleaned dataset size: 50000\n\n--- Performing Stratified 80/10/10 Split ---\nFound 9 unique genres for stratification.\nCasting 'genre' column to ClassLabel type...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1881586098d841c388df4c2374f815ba"}},"metadata":{}},{"name":"stdout","text":"Casting complete.\nTrain split size: 40000\nValidation split size: 5000\nTest split size: 5000\n\nApplying formatting and normalization to all splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835f1895441840539125cd53fff5cb38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46df53fc8904bd7b175a62e6f4b53d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b169c7b2074722a0f0a3f36b67aa5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c8b890d41b4c1ca15f9b68e99a2635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542364e61e7a4000a1515034d0acb817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a14b36d950f482c8fc65a55ce46a7b0"}},"metadata":{}},{"name":"stdout","text":"\n--- Defining Tokenization ---\n\n--- Tokenizing Dataset (all splits) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/35141 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdb1458aabdc4a969ed20d2b75722c3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7188b62152546a59f770cdd5bdac2ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d9161ccfa844018d93791ea99edab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/35141 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0ab849394d45099fffa3ebcc0a8f1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6b8bb9b19e4e3e8b941ea54c1c41fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503809c2e878421c896e1da12501de99"}},"metadata":{}},{"name":"stdout","text":"Tokenization and label setting complete.\n\n--- Configuring Trainer ---\nCalculated steps per epoch: 1099\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"Trainer initialized.\n\n--- Starting Model Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3297' max='3297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3297/3297 2:23:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1099</td>\n      <td>2.281300</td>\n    </tr>\n    <tr>\n      <td>2198</td>\n      <td>2.067600</td>\n    </tr>\n    <tr>\n      <td>3297</td>\n      <td>2.011100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--- Training Finished ---\nSaving final model...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Model saved to ./final_model\n\n--- DELIVERABLE: Sample Generations (Qualitative) ---\nLoaded final-trained model from ./final_model\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nBoth `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Generation Example 1 ---\nPROMPT:\ntitle: spicy chicken pasta\ningredients: chicken breast, pasta, cayenne pepper, olive oil, garlic, tomatoes\nrecipe:\n\n","output_type":"stream"},{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"GENERATED RECIPE:\n 1. put chicken breasts and garlic in a large skillet. 2. add tomatoes, oil and cumin and cook over medium heat, stirring occasionally, until tomatoes are tender. 3. stir in chicken. 4. transfer to a bowl and cover with a tight fitting lid. 5. let stand until ready to use. 6. serve over pasta. 7. serves 8. 8 ounces. 9. makes 12 servings. 10. yields 6 servings, about 12 to 14 servings each. 11. calories: 175. 12. fat: 15. sodium: 7g. 13. cholesterol: 14. carbohydrates: 2g 14 mg. 15 mg 16. fiber: 12g 17. protein: 4g 18. carbohydrate: 3g 19. sugar: 13mg 20. potassium: 5mg 21. calcium: 8mg 22. iron: 6mg 23. vitamin: 20mg 24. zinc: 9mg 25. folate: 10mg 26. selenium: 11mg 27. folic acid: 24mg 28. copper: 25mg 29. vitamins: 30mg 31. polyunsaturated: 35mg 32. dietary fiber of 1g 33. total cholesterol of 2mg 34. blood sugar of 3mg 35. glucose of 20g 36. car\n\n--- Generation Example 2 ---\nPROMPT:\ntitle: black bean and turkey chili\ningredients:\nrecipe:\n\n","output_type":"stream"},{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"GENERATED RECIPE:\n cook, black beans, ground cumin, 2, white rice, about 15 minutes, garlic, tomatoes, 15 to 20 minutes\nRECIPE: 1. cook the black and ground black pepper in a heavy-bottomed saucepan over medium heat, stirring occasionally, until the peppers are golden brown, 10 minutes. 2. remove the pan from the heat and add the garlic and 1 teaspoon salt. 3. bring the mixture to a boil, then reduce the sauce to medium-low and simmer for 15-20 minutes or until most of the liquid is absorbed. 4. drain the beans and reserve for another use. 5. to make the chili: combine the remaining 2 teaspoons salt and black peppercorns in an electric blender and puree until smooth. 6. add 1 tablespoon oil and continue to pureer until you have a coarse powder. 7. strain the oil through a fine mesh strainer or a food mill. 8. when the spice mixture is thoroughly mixed, add it to the stew and cook until heated through, adding more oil if necessary. 9. serve with tortilla chips or other condiments. 10. if you like, heat up a grill for a few minutes to serve. 11. note: the recipe makes about 8 servings. 12\n\n--- Generation Example 3 ---\nPROMPT:\ntitle:\ningredients: arugula, pomegranate arils, persimmon, feta cheese, walnuts, vinaigrette\nrecipe:\n\nGENERATED RECIPE:\n 1. cut and serve. 2. i like to use fresh pears for this recipe. 3. they can be eaten with the remaining ingredients. 4. you can also use any of the other ingredients for the salad. 5. it's great to have a few extra pints of dressing at the table. 6. note: you could also add more fetas or walnut halves for a healthier version. 7. the dressing will keep for several days in the refrigerator. 8. this is a quick and easy salad and very good for parties. 9. can make ahead: 10. in a large bowl, combine the pita bread and wal nuts. 11. toss well. 12. pour dressing over the bread, sprinkle with salt and pepper and toss again. 13. sprinkle another coat on top and spread the rest. 14. serve with any dressing you choose. 15. enjoy! 16.! 17. notes: 18.  19. 1) it can go great with french toast, or you want a great salad with a bit of green onion or a little of cheese. 20. (i like a sweet one, but not too sweet.) 21. if you prefer a more fruity flavor, try a lighter version or omit the cheese altogether. 22\n\n--- DELIVERABLE: Metric Evaluation (ROUGE, BLEU) ---\n*** Running evaluation on the 10% held-out TEST set ***\nRunning quantitative evaluation on 100 test samples...\nEvaluation generation complete. Calculating scores...\nNo predictions were generated. Skipping metric evaluation.\n\n--- SCRIPT COMPLETE ---\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# %%capture\n# 1. INSTALL NECESSARY LIBRARIES\n!pip install transformers[torch] datasets accelerate evaluate rouge_score nltk scikit-learn\n\n# 2. IMPORTS\nimport torch\nimport pandas as pd\nimport numpy as np\nimport ast  # For safely evaluating string representations of lists\nimport evaluate  # For ROUGE and BLEU\nimport math # For calculating steps per epoch\nimport os   # For checking if model exists\nfrom datasets import load_dataset, Dataset, DatasetDict, ClassLabel, Features\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 3. GLOBAL CONFIGURATION AND CONSTANTS\n# --- Parameters You Can Tune ---\nMODEL_NAME = 'gpt2'\nFILE_PATH = '/kaggle/input/3a2mext/3A2M_EXTENDED.csv'\nFINAL_MODEL_PATH = \"./final_model\" # Path to save/load the trained model\n\n# Training parameters\nTRAIN_EPOCHS = 3\nFP16_TRAINING = True\nBATCH_SIZE = 4       # Small batch size for training and eval\nGRAD_ACC_STEPS = 8\nMAX_LENGTH = 512\n\n# Data parameters\nDATA_SUBSAMPLE = 50000\nVAL_SIZE = 0.1  # 10% for validation\nTEST_SIZE = 0.1  # 10% for test (80% for train)\n# ----------------------------------\n\nprint(\"--- Configuration Loaded ---\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Data Subsample: {DATA_SUBSAMPLE} rows\")\nprint(f\"Split: 80% Train, 10% Validation, 10% Test\")\nprint(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACC_STEPS}\")\nprint(\"----------------------------\")\n\n# 4. LOAD MODEL AND TOKENIZER (FOR TRAINING)\nprint(\"\\n--- Loading Model and Tokenizer ---\")\n# This tokenizer uses default RIGHT padding, which is correct for training\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\nmodel.resize_token_embeddings(len(tokenizer))\nprint(\"Model and Tokenizer loaded successfully.\")\n\n# 5. LOAD, CLEAN, AND DEDUPLICATE DATASET\nprint(\"\\n--- Loading, Cleaning, and Deduplicating Dataset ---\")\ntry:\n    raw_dataset = load_dataset('csv', data_files=FILE_PATH, split='train')\n    print(f\"Original dataset size: {len(raw_dataset)}\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}. Please check the file path.\")\n    data = {'title': ['Test 1', 'Test 2', 'Test 3', 'Test 4'],\n            'Extended_NER': [\"['a']\", \"['b']\", \"['c']\", \"['d']\"],\n            'directions': [\"['step 1']\", \"['step 2']\", \"['step 3']\", \"['step 4']\"],\n            'genre': ['vegetables', 'sides', 'nonveg', 'vegetables']}\n    raw_dataset = Dataset.from_dict(data)\n\nif DATA_SUBSAMPLE > 0 and DATA_SUBSAMPLE < len(raw_dataset):\n    raw_dataset = raw_dataset.shuffle(seed=42).select(range(DATA_SUBSAMPLE))\n    print(f\"Subsampled dataset to {len(raw_dataset)} rows.\")\n\noriginal_count = len(raw_dataset)\nraw_dataset = raw_dataset.filter(\n    lambda x: x['title'] is not None and \\\n              x['Extended_NER'] is not None and \\\n              x['directions'] is not None and \\\n              x['genre'] is not None\n)\nprint(f\"Filtered {original_count - len(raw_dataset)} rows with null values.\")\n\ndef clean_genre(example):\n    example['genre'] = str(example['genre']).lower().strip()\n    return example\nraw_dataset = raw_dataset.map(clean_genre)\nprint(\"Normalized 'genre' column.\")\n\ndf = raw_dataset.to_pandas()\noriginal_count = len(df)\ndf.drop_duplicates(subset=['title', 'Extended_NER', 'directions'], inplace=True, keep='first')\nprint(f\"Removed {original_count - len(df)} duplicate recipes.\")\nraw_dataset = Dataset.from_pandas(df)\nprint(f\"Final cleaned dataset size: {len(raw_dataset)}\")\n\n# 6. STRATIFIED 80/10/10 SPLIT\nprint(\"\\n--- Performing Stratified 80/10/10 Split ---\")\nunique_genres = raw_dataset.unique(\"genre\")\nprint(f\"Found {len(unique_genres)} unique genres for stratification.\")\ncurrent_features = raw_dataset.features\nnew_features = Features({\n    **current_features,\n    \"genre\": ClassLabel(names=unique_genres)\n})\nprint(\"Casting 'genre' column to ClassLabel type...\")\nstratify_ready_dataset = raw_dataset.cast(new_features)\nprint(\"Casting complete.\")\ntrain_test_split = stratify_ready_dataset.train_test_split(\n    test_size=(VAL_SIZE + TEST_SIZE),\n    seed=42,\n    stratify_by_column=\"genre\"\n)\nval_test_split = train_test_split['test'].train_test_split(\n    test_size=0.5,\n    seed=42,\n    stratify_by_column=\"genre\"\n)\nsplit_dataset = DatasetDict({\n    'train': train_test_split['train'],\n    'validation': val_test_split['train'],\n    'test': val_test_split['test']\n})\nprint(f\"Train split size: {len(split_dataset['train'])}\")\nprint(f\"Validation split size: {len(split_dataset['validation'])}\")\nprint(f\"Test split size: {len(split_dataset['test'])}\")\n\n# 7. DEFINE FORMATTING FUNCTION (WITH ENHANCED CLEANING)\ndef format_and_normalize_record(example):\n    try:\n        ingredients_list = ast.literal_eval(example['Extended_NER'])\n        if not isinstance(ingredients_list, list): ingredients_list = []\n    except:\n        ingredients_list = []\n    ingredients_str = \", \".join(filter(None, [ing.lower().strip() for ing in ingredients_list]))\n\n    try:\n        directions_list = ast.literal_eval(example['directions'])\n        if not isinstance(directions_list, list): directions_list = []\n    except:\n        directions_list = []\n    \n    directions_list_clean = [str(s).lower().strip() for s in directions_list]\n    filtered_steps = [\n        step for step in directions_list_clean if not \n        (step.startswith(\"calories:\") or\n         step.startswith(\"fat:\") or\n         step.startswith(\"sodium:\") or\n         step.startswith(\"cholesterol:\") or\n         step.startswith(\"protein:\") or\n         step.startswith(\"carbohydrates:\") or\n         \"serving size\" in step or\n         step.isnumeric())\n    ]\n    \n    directions_str = \" \".join(f\"{i+1}. {step}\" for i, step in enumerate(filtered_steps))\n    title_str = str(example['title']).strip().lower()\n\n    if not title_str or not ingredients_str or not directions_str:\n        return {\"text\": None}\n\n    text = (\n        f\"TITLE: {title_str}\\n\"\n        f\"INGREDIENTS: {ingredients_str}\\n\"\n        f\"RECIPE: {directions_str}\"\n        f\"{tokenizer.eos_token}\"\n    )\n    return {\"text\": text}\n\n# 8. APPLY FORMATTING\nprint(\"\\nApplying formatting and normalization to all splits...\")\nformatted_dataset = split_dataset.map(\n    format_and_normalize_record,\n    num_proc=4,\n    remove_columns=split_dataset['train'].column_names\n)\nformatted_dataset = formatted_dataset.filter(\n    lambda example: example['text'] is not None\n)\n\n# 9. DEFINE TOKENIZATION FUNCTION (WITH PADDING FIX)\nprint(\"\\n--- Defining Tokenization ---\")\ndef tokenize_function(examples):\n    # This uses the default RIGHT padding, which is correct for training.\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_LENGTH,\n    )\n    \n# 10. APPLY TOKENIZATION AND SET LABELS\nprint(\"\\n--- Tokenizing Dataset (all splits) ---\")\ntokenized_dataset = formatted_dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=[\"text\"]\n)\ndef set_labels(examples):\n    examples[\"labels\"] = examples[\"input_ids\"].copy()\n    return examples\ntokenized_dataset = tokenized_dataset.map(set_labels, batched=True, num_proc=4)\nprint(\"Tokenization and label setting complete.\")\n\n# 11. CONFIGURE TRAINER (WITH ALL VERSIONING FIXES)\nprint(\"\\n--- Configuring Trainer ---\")\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\ntry:\n    total_train_samples = len(tokenized_dataset[\"train\"])\n    effective_batch_size = BATCH_SIZE * GRAD_ACC_STEPS\n    STEPS_PER_EPOCH = math.ceil(total_train_samples / effective_batch_size)\n    print(f\"Calculated steps per epoch: {STEPS_PER_EPOCH}\")\nexcept:\n    print(\"Warning: Could not calculate steps_per_epoch. Defaulting to 500.\")\n    STEPS_PER_EPOCH = 500\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-recipe-generator\",\n    overwrite_output_dir=True,\n    num_train_epochs=TRAIN_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACC_STEPS,\n    fp16=FP16_TRAINING,\n    do_eval=True,\n    eval_steps=STEPS_PER_EPOCH,\n    save_steps=STEPS_PER_EPOCH,\n    logging_steps=STEPS_PER_EPOCH,\n    dataloader_num_workers=0,\n    learning_rate=5e-5,\n    warmup_steps=200,\n    weight_decay=0.01,\n    report_to=\"none\",\n)\n\n# 12. INITIALIZE TRAINER\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    data_collator=data_collator,\n)\nprint(\"Trainer initialized.\")\n\n# 13. TRAIN THE MODEL (WITH CHECKPOINTING)\nprint(\"\\n--- Starting Model Training ---\")\nif not os.path.exists(FINAL_MODEL_PATH):\n    print(f\"No model found at {FINAL_MODEL_PATH}. Starting training...\")\n    try:\n        trainer.train()\n        print(\"--- Training Finished ---\")\n        print(\"Saving final model...\")\n        trainer.save_model(FINAL_MODEL_PATH)\n        tokenizer.save_pretrained(FINAL_MODEL_PATH)\n        print(f\"Model saved to {FINAL_MODEL_PATH}\")\n    except Exception as e:\n        print(f\"\\n--- AN ERROR OCCURRED DURING TRAINING ---\")\n        print(f\"ERROR: {e}\")\n        print(\"\\nThis error is often due to OOM or data collation.\")\nelse:\n    print(f\"--- Model already found at {FINAL_MODEL_PATH}. Skipping training. ---\")\n\n# 14. LOAD FINE-TUNED MODEL FOR INFERENCE\nprint(\"\\n--- DELIVERABLE: Loading Model for Evaluation ---\")\ntry:\n    final_model = GPT2LMHeadModel.from_pretrained(FINAL_MODEL_PATH)\n    \n    # --- FIX: Load tokenizer and set padding_side='left' for generation ---\n    final_tokenizer = GPT2Tokenizer.from_pretrained(FINAL_MODEL_PATH)\n    final_tokenizer.padding_side = 'left' \n    # --- End of Fix ---\n    \n    print(f\"Loaded final-trained model from {FINAL_MODEL_PATH}\")\nexcept Exception as e:\n    print(f\"Error loading model from {FINAL_MODEL_PATH}: {e}\")\n    print(\"Loading base model as a fallback.\")\n    final_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n    \n    # --- FIX: Load tokenizer and set padding_side='left' for generation ---\n    final_tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n    final_tokenizer.padding_side = 'left'\n    final_tokenizer.pad_token = final_tokenizer.eos_token # Set pad token for base tokenizer\n    # --- End of Fix ---\n\n# We must ensure the pad token is set for the inference tokenizer\nfinal_tokenizer.pad_token = final_tokenizer.eos_token\n\nrecipe_generator = pipeline(\n    \"text-generation\",\n    model=final_model,\n    tokenizer=final_tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\n# 15. --- NEW: QUALITATIVE GENERATION FROM TEST SET ---\nprint(\"\\n--- DELIVERABLE: Sample Generations (from Test Set) ---\")\n\n# Select 5 random samples from the test set\ntry:\n    sample_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(5))\nexcept:\n    print(\"Test set is smaller than 5. Using all available samples.\")\n    sample_dataset = tokenized_dataset[\"test\"]\n\nfor i, example in enumerate(sample_dataset):\n    # Decode, and also clean up any padding tokens\n    full_text = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n    \n    prompt_text = \"\"\n    reference_text = \"\"\n    \n    try:\n        parts = full_text.split(\"RECIPE:\") \n        prompt_text = parts[0] + \"RECIPE:\"\n        reference_text = parts[1].strip()\n    except:\n        print(f\"Error splitting sample {i+1}. Skipping.\")\n        continue \n    \n    # Generate a recipe\n    generated_output = recipe_generator(\n        prompt_text,\n        max_new_tokens=150,\n        eos_token_id=final_tokenizer.eos_token_id,\n        pad_token_id=final_tokenizer.eos_token_id,\n        no_repeat_ngram_size=2,\n        temperature=0.7,\n        top_k=50\n    )\n    \n    # Clean the output\n    full_gen_text = generated_output[0]['generated_text'] \n    predicted_text = full_gen_text[len(prompt_text):].strip()\n    if final_tokenizer.eos_token in predicted_text:\n        predicted_text = predicted_text.split(final_tokenizer.eos_token)[0]\n\n    # Print the comparison\n    print(f\"\\n--- SAMPLE {i+1} ---\")\n    print(f\"PROMPT:\\n{prompt_text}\\n\")\n    print(f\"GENERATED:\\n{predicted_text}\\n\")\n    print(f\"REFERENCE (Actual Recipe):\\n{reference_text}\\n\")\n    print(\"-\" * 30)\n\n# 16. SETUP QUANTITATIVE EVALUATION (ON TEST SET)\nprint(\"\\n--- DELIVERABLE: Metric Evaluation (ROUGE, BLEU) ---\")\nprint(\"*** Running evaluation on the 10% held-out TEST set ***\")\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\neval_sample_size = 100\nif len(tokenized_dataset[\"test\"]) > eval_sample_size:\n    eval_sample = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(eval_sample_size))\nelse:\n    eval_sample = tokenized_dataset[\"test\"]\nprint(f\"Preparing {len(eval_sample)} samples for batch evaluation...\")\n\n# 17. RUN QUANTITATIVE EVALUATION (EFFICIENT BATCH MODE)\nprint(\"Building prompts and references list...\")\nprompts_list = []\nreferences_list = []\n\nfor example in eval_sample:\n    token_ids = example['input_ids']\n    full_text = tokenizer.decode(token_ids, skip_special_tokens=True)\n    try:\n        parts = full_text.split(\"RECIPE:\") \n        prompt_text = parts[0] + \"RECIPE:\"\n        reference_text = parts[1].strip()\n        prompts_list.append(prompt_text)\n        references_list.append(reference_text)\n    except:\n        continue \n\nprint(f\"Generating {len(prompts_list)} predictions in a single batch...\")\ngenerated_outputs = recipe_generator(\n    prompts_list,\n    max_new_tokens=150,\n    eos_token_id=final_tokenizer.eos_token_id,\n    pad_token_id=final_tokenizer.eos_token_id,\n    no_repeat_ngram_size=2,\n    batch_size=BATCH_SIZE\n)\n\nprint(\"Processing generated outputs...\")\npredictions = []\nfor i, output in enumerate(generated_outputs):\n    full_gen_text = output[0]['generated_text'] \n    prompt_text = prompts_list[i]\n    predicted_text = full_gen_text[len(prompt_text):].strip()\n    if final_tokenizer.eos_token in predicted_text:\n        predicted_text = predicted_text.split(final_tokenizer.eos_token)[0]\n    predictions.append(predicted_text.strip())\n\nreferences = references_list\n\nprint(f\"Evaluation generation complete. Processed {len(predictions)} samples.\")\n\n# 18. DISPLAY EVALUATION RESULTS AND FINAL NOTES\nif predictions:\n    rouge_scores = rouge.compute(predictions=predictions, references=references)\n    bleu_scores = bleu.compute(predictions=predictions, references=references)\n\n    print(\"\\n--- Final Evaluation Results (on TEST set) ---\")\n    print(\"\\nROUGE Scores:\")\n    print(f\"  ROUGE-1: {rouge_scores['rouge1'] * 100:.2f}\")\n    print(f\"  ROUGE-2: {rouge_scores['rouge2'] * 100:.2f}\")\n    print(f\"  ROUGE-L: {rouge_scores['rougeL'] * 100:.2f}\")\n\n    print(\"\\nBLEU Score:\")\n    print(f\"  BLEU: {bleu_scores['bleu'] * 100:.2f}\")\n\nelse:\n    print(\"No predictions were generated. Skipping metric evaluation.\")\n\nprint(\"\\n--- SCRIPT COMPLETE ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:08:07.723721Z","iopub.execute_input":"2025-11-01T17:08:07.724680Z","iopub.status.idle":"2025-11-01T17:10:27.843015Z","shell.execute_reply.started":"2025-11-01T17:08:07.724646Z","shell.execute_reply":"2025-11-01T17:10:27.842144Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\nRequirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers[torch]) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.8.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers[torch]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\n--- Configuration Loaded ---\nModel: gpt2\nData Subsample: 50000 rows\nSplit: 80% Train, 10% Validation, 10% Test\nEffective Batch Size: 32\n----------------------------\n\n--- Loading Model and Tokenizer ---\nModel and Tokenizer loaded successfully.\n\n--- Loading, Cleaning, and Deduplicating Dataset ---\nOriginal dataset size: 2231143\nSubsampled dataset to 50000 rows.\nFiltered 0 rows with null values.\nNormalized 'genre' column.\nRemoved 0 duplicate recipes.\nFinal cleaned dataset size: 50000\n\n--- Performing Stratified 80/10/10 Split ---\nFound 9 unique genres for stratification.\nCasting 'genre' column to ClassLabel type...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be2cf91099a3494698e40e710e400c21"}},"metadata":{}},{"name":"stdout","text":"Casting complete.\nTrain split size: 40000\nValidation split size: 5000\nTest split size: 5000\n\nApplying formatting and normalization to all splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b95f01cf708478e8d026a892acfc6f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854cb30f3d7146d4b9d836aba5b3c44c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b87b4907864bdd93aab4cef60fd7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b1e741ae204e84b73a9a112988fe66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef51939903b4bb2a690cf2bcc2a8e3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e981665442c9494aa86f562362a3b593"}},"metadata":{}},{"name":"stdout","text":"\n--- Defining Tokenization ---\n\n--- Tokenizing Dataset (all splits) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/35139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820f5d2b5d3e473bae7d7bcc7fb4fe21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ed01a04a0c4a24b949643b0a725adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb17db83a1b4be79af913dfacefe86e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/35139 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e6838058df44c3852d93834f57ee5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f703321d856f4f69b3f78ced511b8d47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746e9a7316b546938caae5549ee01182"}},"metadata":{}},{"name":"stdout","text":"Tokenization and label setting complete.\n\n--- Configuring Trainer ---\nCalculated steps per epoch: 1099\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Trainer initialized.\n\n--- Starting Model Training ---\n--- Model already found at ./final_model. Skipping training. ---\n\n--- DELIVERABLE: Loading Model for Evaluation ---\nLoaded final-trained model from ./final_model\n\n--- DELIVERABLE: Sample Generations (from Test Set) ---\n\n--- SAMPLE 1 ---\nPROMPT:\nTITLE: joyce's mexican corn bread\nINGREDIENTS: 40-45 minutes, bell pepper, grease, salt, 8 inch square, 350 degrees, cornmeal, bake, peppers, creamstyle corn, 12, grated cheese, sour cream, 14, cooking oil, eggs\nRECIPE:\n\nGENERATED:\n1. preheat oven to 350. grease and flour 8 inches square baking pan. 2. combine cream cheese and sour creamed corn. 3. add eggs, one at a time, beating well after each addition. 4. stir in grates and pepper. 5. pour mixture into pan and bake for 40 to 45 minutes or until golden brown. 6. cool slightly. 7. for the filling, combine sour and cream and spread over corn mixture. 8. bake until filling is bubbly and corn is golden, about 5-7 minutes. 9. serve warm or at room temperature. 10. makes 12 servings. 11. notes: you can substitute sour corn or cream for sour dip. 12. sour crackers are good too\n\nREFERENCE (Actual Recipe):\n1. grease an 8 inch square pan. or a 12 cup muffin tin, or 14 stick pan. 2. combine all ingredients. 3. pour into prepared pan. bake at 350 degrees for 40-45 minutes.\n\n------------------------------\n\n--- SAMPLE 2 ---\nPROMPT:\nTITLE: raspberry-rhubarb crisp\nINGREDIENTS: ground ginger, serve, salt, bake, raspberries, cold butter, sugar, walnuts, rhubarb, 3, cover, 350\\u00b0, ground cinnamon, about 45 minutes, cornstarch, allpurpose, combine, add, regular rolled oats, brown sugar\nRECIPE:\n\nGENERATED:\n1. preheat the oven to 350. 2. combine rhizomes, butter and sugar in a large bowl. add the oats and toss to coat, tossing to combine. pour the batter into a greased 9 x 13-inch baking dish and bake until a toothpick inserted in the center comes out clean, 45 to 60 minutes. 3. remove from the baking sheet and let cool completely. 4. meanwhile, in another large saucepan, melt the butter over medium-high heat. stir in all of the ingredients, except the ritzberries. cook, stirring occasionally, until the mixture begins to thicken, but not completely, remove to a plate; pour over the rhizo mixture. cover and chill until ready to serve\n\nREFERENCE (Actual Recipe):\n1. preheat oven to 350. combine oats, flour, walnuts, brown sugar, cinnamon, ginger, and salt in a large bowl. with your fingers, rub butter into oat mixture until blended and coarse lumps form. cover and chill. 2. combine granulated sugar and cornstarch in another large bowl. add raspberries and rhubarb and toss gently to combine with cornstarch mixture. pour into a shallow 2- to 3-qt. baking dish and sprinkle evenly with topping. 3. bake crisp until topping is golden brown and 4. is bubbling, about 45 minutes. serve warm or at room temperature, with ice cream.\n\n------------------------------\n\n--- SAMPLE 3 ---\nPROMPT:\nTITLE: chicken, quinoa, and green-olive stew\nINGREDIENTS: salt, quinoa, chicken broth, extravirgin olive oil, chicken, wipe, cook, about 10 minutes, pimiento, ground cumin, 15 to 20 minutes, ancho chile powder, white onion, 10 to 15 minutes, orange zest, add, oregano, garlic, reduce, tomatoes, cayenne, 2 minutes, ground coriander\nRECIPE:\n\nGENERATED:\n1. in a large saucepan, heat olive and garlic oil over medium-high heat until fragrant. 2. add onion and cook until golden, breaking up any brown bits. 3. stir in quesadillas, zests, 1 teaspoon cilantro, green onions, salt and caryenne pepper. 4. bring to a boil and reduce heat. 5. reduce the heat to low, cover, simmer 15 seconds, stirring frequently. 6. remove from heat and let stand, covered, until quevo is tender, at least an hour. 7. transfer quell to colander and cover. 8. heat additional stock in large skillet over low heat, bring the queros to high. 9. cook quebec\n\nREFERENCE (Actual Recipe):\n1. bring broth to a simmer in a large, heavy-bottomed pot over medium-high heat. add chicken and lower heat to a simmer. cook chicken, covered, 15 to 20 minutes, or until cooked through; transfer to a plate. pour broth into a large bowl and set aside. wipe out pot. 2. add oil, onion, and salt to pot and cook over medium heat until onion softens and is starting to brown, about 10 minutes. 3. stir in cumin, coriander, oregano, and garlic; cook 2 minutes. add ancho chile powder, cayenne, chopped tomatoes, reserved broth, orange zest, and quinoa. reduce heat and simmer, covered, until a white ring appears around each quinoa seed, 10 to 15 minutes. meanwhile, shred chicken. 4. add shredded chicken, chickpeas, and olives and heat through. 5. note: nutritional analysis is per serving.\n\n------------------------------\n\n--- SAMPLE 4 ---\nPROMPT:\nTITLE: pureed broccoli\nINGREDIENTS: russet potatoes, fresh dill, whipping cream, 5 minutes, about 15 minutes, leeks, butter, broccoli, add leeks, cook\nRECIPE:\n\nGENERATED:\n1. heat a large skillet over medium-high heat. 2. add the broccoli and cook, stirring once, until wilted, 4 to 5 to 6 minutes. remove from heat and let cool. 3. in a small bowl, combine the butter and cream. 4. whisk in the diced broccoli. 5. stir in milk and the seasonings. 6. pour over broccoli mixture. 7. cook until golden, turning once. (this will take about 5 min.) 8. transfer to a plate. 9. serve with a dollop of sour cream and a scoop of the crumbled bacon. 10. i like to dip in some of my favorite sesame seeds and sprinkle the whole thing over the top. 11. it's\n\nREFERENCE (Actual Recipe):\n1. melt butter in a large skillet over medium heat. 2. stir in leeks. 3. cover and stir often until leeks are very soft (about 15 minutes). 4. cook potato in salted water until almost done (about 15 minutes). 5. to potato and water, add broccoli stems (5 minutes) and then florets (5 minutes) or until tender. 6. drain well and transfer to food processor. 7. add leeks to the food processor and puree all. 8. return puree to skillet and add cream, dill, salt and pepper. 9. blend well and serve.\n\n------------------------------\n\n--- SAMPLE 5 ---\nPROMPT:\nTITLE: beer n' butter poultry injection\nINGREDIENTS: butter, salt, onion powder, garlic, tabasco sauce, worcestershire sauce, soy sauce, beer, stir\nRECIPE:\n\nGENERATED:\n1. in a large saucepan, melt butter over medium heat. 2. add garlic and onion and saute until tender. 3. stir in beer and worchestershire. 4. pour over chicken and stir until coated. 5. cover and simmer on low for 10 minutes. 6. serve hot or at room temperature. 7. serves 4 to 6 people. 8. makes 4 servings. 9. can be made ahead of time. 10. this recipe has been provided free of charge. 11. for more information about making this soup, go to http://www.russianvietnam.com/recipe/recipes/vibh/1.html, and then go back to www.myvibe.\n\nREFERENCE (Actual Recipe):\n1. combine all ingredients in a sauce pan over a low heat. stir and heat until salt is disolved and the sauce is even and runny. keep warm (but not hot) to inject.\n\n------------------------------\n\n--- DELIVERABLE: Metric Evaluation (ROUGE, BLEU) ---\n*** Running evaluation on the 10% held-out TEST set ***\nPreparing 100 samples for batch evaluation...\nBuilding prompts and references list...\nGenerating 100 predictions in a single batch...\nProcessing generated outputs...\nEvaluation generation complete. Processed 100 samples.\n\n--- Final Evaluation Results (on TEST set) ---\n\nROUGE Scores:\n  ROUGE-1: 31.94\n  ROUGE-2: 5.86\n  ROUGE-L: 17.35\n\nBLEU Score:\n  BLEU: 7.43\n\n--- SCRIPT COMPLETE ---\n","output_type":"stream"}],"execution_count":27}]}